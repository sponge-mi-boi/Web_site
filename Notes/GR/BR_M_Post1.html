<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
    <title>1</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>

    <link rel="stylesheet" href="../../te.css">
    <script src="../../marked.min.js"></script>
    <script src ='../../script.js'></script>
    <!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
    <script type="text/x-mathjax-config">
        init_mathjax = function() {
            if (window.MathJax) {
            // MathJax loaded
                MathJax.Hub.Config({
                    TeX: {
                        equationNumbers: {
                        autoNumber: "AMS",
                        useLabelIds: true
                        }
                    },
                    tex2jax: {
                        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                        processEscapes: true,
                        processEnvironments: true
                    },
                    displayAlign: 'center',
                    messageStyle: 'none',
                    CommonHTML: {
                        linebreaks: {
                        automatic: true
                        }
                    }
                });

                MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
            }
        }
        init_mathjax();
    </script>
    <!-- End of mathjax configuration -->
<body class="notes">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=a1be5abf-b872-4958-88b4-aeff9c66616b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Quick-Note">Quick Note<a class="anchor-link" href="#Quick-Note"></a></h2><p>This will be a slightly unique section in that GR in its true formulation seems mathematically complex. However, the equivalence of all frames of reference, inertial or non-inertial, is the key to understanding the root of GR. This principle, which implies the metric of Lorentzian spacetime, $\eta$, exists upon any local coordinate system for any global metric $g$ means that essentially whatever conclusions are drawn in SR can be extended to GR.</p>
<h2 id="Index-Notation">Index Notation<a class="anchor-link" href="#Index-Notation"></a></h2><p>Index formatting is very common and the easiest way to express quantities in GR. Take a vector $v$ in a vector space $V$ of $n$ dimensions. Consider a matrix $A$ which represents a linear transformation on $V$ such that $A : V \rightarrow V$. The components of $V$ are $v^\alpha$ where $\alpha \in \{0,...n - 1\}$. Common matrix-vector mulitplication is represented as follows $v = \begin{bmatrix} v^0 \\ \vdots\\ v^{n-1} \end{bmatrix}, A = \begin{bmatrix} A^{00} &amp; ... &amp; A^{0n} \\ \vdots&amp; ... &amp; \vdots\\ A^{(n-1)(0)} &amp; ... &amp; A^{(n-1)(n-1)} \end{bmatrix}$</p>
<p>$\rightarrow v' =\begin{bmatrix} v'^0 \\ \vdots\\ v'^{n-1} \end{bmatrix} = A v = \begin{bmatrix} A^{00} &amp; ... &amp; A^{0(n - 1)} \\ \vdots&amp; ... &amp; \vdots\\ A^{(n-1)(0)} &amp; ... &amp; A^{(n-1)(n-1)} \end{bmatrix} \begin{bmatrix} v^0 \\ \vdots\\ v^{n-1} \end{bmatrix} $</p>
<p>$ = \begin{bmatrix}A^{00}v^0 + ... + A^{0(n-1)}v^{n-1} \\ \vdots\\ A^{(n-1)0}v^0 + ... + A^{(n-1)(n-1)}v^{n-1}  \end{bmatrix}$</p>
<p>It is inefficient to explicitly write every time if there are many calculations being done or there are a lot of components. Using index notation, $v' = Av \rightarrow v'^\alpha = A^\alpha_\beta v^\beta $, and repeated indices, which is the $\beta$ in this case, is summed over. $\alpha$ represents the component of the vector and can range from the zeroth component to the $(n-1)$th component where n is the dimensionality of the vector space.</p>
<p>The inverse of a matrix is then $A A^{-1} = I \rightarrow A^\alpha_\beta {(A)^{-1}}^\beta_\gamma = \delta^\alpha_\gamma$. The Kronecker delta $\delta$ is merely the Identity matrix in components $\delta^u_v = 0$ if $ u \neq v, $ else $1$.</p>
<h2 id="A-Closer-Look-at-Vectors">A Closer Look at Vectors<a class="anchor-link" href="#A-Closer-Look-at-Vectors"></a></h2><p>In index notation, a vector $v$ has components $v^\alpha$. However, consider a basis of vectors $\{e_\alpha\}$ where $\alpha \in \{0,...n - 1\}$. The vector $v$ in this basis can be represented as $v = v^\alpha e_\alpha$ where the repeated index $\alpha$ is summed over just like above. Consider the $x,y,z$ basis in 3 dimensional Euclidean space. The basis vectors are usually indicated as the unit vectors in each of the 3 spatial directions and any other vector can be represented as a linear combination of these three. This is written as $V = Span\{\hat{x},\hat{y},\hat{z}\}$, meaning these vectors span the entire vector space $V$.</p>
<p>Now, consider partial derivatives $\frac{\partial }{\partial x} = \partial_x, \frac{\partial }{\partial y}= \partial_y, \frac{\partial }{\partial z}= \partial_z$. If there exists a scalar function $f = f(x,y,z)$, it should be familiar from vector calculus that $\partial_x f$ represents the change of $f$ in the x direction. Similar results hold for the other directions, meaning that each partial derivative $\partial_i \in \{\partial_x,\partial_y,\partial_z\}$ is uniquely associated with a basis vector $e^i$. Therefore, it makes sense in a geometrical way to define each basis vector with each partial derivative $e_i \equiv \partial_i$. In this manner one can view a vector $v = v^\alpha e_\alpha$ as a function $v = v^\alpha \partial_\alpha$.</p>
<h3 id="Dual-Vectors">Dual Vectors<a class="anchor-link" href="#Dual-Vectors"></a></h3><p>This perspective makes further sense when one considers the concept of dual vectors. The dual vector space $V^*$ of a vector space $V$ is the set of all possible linear functions $V^*: V \rightarrow R$, from the vector space to the set of real numbers. Is this really a vector space?</p>
<p>Well, recall the definition of a vector space. It is a set $V$ such that $v_3 = v_1 + v_2 \in V , \forall v_1,v_2 \in V$ and $rv = r (v^\alpha e_\alpha) = (r v^\alpha) e_\alpha = {v'}^\alpha e_\alpha = v' \in V, \forall v \in V, r \in \mathcal{R}$, or in other words, vector addition and scalar vector multiplication holds.</p>
<p>Consider an element $v^*$ in $V^*$. Linearity ensures ${v_1}^*(v) + {v_2}^*(v) = ({v_1}^* + {v_2}^*)(v), \forall v \in V$ and $(rv^*)v = v^*(rv) \forall v,r \in V, \mathcal{R}$. The dual space is easily shown to be a vector space. Therefore, one can define a basis $\{w^\alpha\}$ in $V^*$ such that $v = v^*_\alpha w^\alpha$. What is the dimensionality of $V^*$?</p>
<p>Well, consider $v^*(v) = {v^*}_\alpha w^\alpha (v^\beta e_\beta)= {v^*}_\alpha v^\beta w^\alpha (e_\beta) \in \mathcal{R} $. For this result to hold for all possible $v,v^*$, there must be 1-1 correspondence between $\{w^\alpha\}$ and $\{e_\alpha\}$. Therefore, $Dim(V) = Dim(V^*)$. However, note that there is no way to actually define this mapping, meaning there is no natural way to define each basis vector with its dual. The existence of a vector space implies the existence of a dual vector space, but there is no natural mapping between them, unless one is defined. However, by normalization, one can define the relation between the vector basis and its duals by $w^\alpha e_\beta = \delta^\alpha_\beta$.</p>
<p>In the above geometrical picture, $\partial_\alpha$ was identified with the basis vectors of $V$. What is a function that takes $\partial_\alpha$ and results in a real number? Consider the differential of a scalar function $df$. By the chain rule, $df = \partial_\alpha f dx^\alpha$ where the $\{dx^\alpha\}$ are the differentials in each of the basis vector directions. Rewriting, $df = dx^\alpha \partial_\alpha f $. A natural 1-1 correspondence arises between partial derivatives and the differentials in each of the basis vector directions $\partial_\alpha \Leftrightarrow dx^\alpha $ in this particular case.
Therefore, in terms of geometry, each of the dual basis vectors $w^\alpha$ are associated with each of the differentials $dx^\alpha$.</p>
<p>An interesting aspect is that while $v^*$ is considered a function which takes  $V$ to $\mathcal{R}$, it is equally justified to consider $v$ as a function which takes $V^*$ to $\mathcal{R}$. This also justifies considering vectors as functions.</p>
<h3 id="Tensors-as-Multilinear-Products">Tensors as Multilinear Products<a class="anchor-link" href="#Tensors-as-Multilinear-Products"></a></h3><p>Now, consider the multilinear function $w^\alpha e_\beta $. This takes as an input a vector and a dual vector concurrently. How many distinct functions are there of this type? Well, it's merely $Dim(V^*)Dim(V) = n^2$. Since each of these functions are linearly independent of each other, this resulting space of multilinear functions created by the combination of a vector space $V$ and its dual space $V^*$ is spanned by these $n^2$ basis functions. Any such element of this space can be defined as $T = T_\alpha^\beta w^\alpha e_\beta $ where $T_\alpha^\beta$ are considered the components of this multilinear function in the basis $\{w^\alpha e_\beta\}, \alpha, \beta \in \{0,...,n-1\}$. This is nothing but the definition of a tensor, which in this case is of dimension $n^2$. The mathematical operation for the combination of the vector and dual vector basis is called the tensor product $w^\alpha e_\beta = w^\alpha \otimes e_\beta$. One can also define the rank of a tensor as $(m,n)$ where m is the number of vectors and n is the number of dual vectors in the basis element of that tensor. The example given above is of rank $(1,1)$.</p>
<p>It is possible as can be easily seen to form tensors of arbitrary rank by merely combining (taking the tensor product) of vectors and dual vectors. The most common tensor in GR is, of course, the metric tensor $g$. This is actually the most fundamental quantity in GR and proposing/finding the form of this metric is the entire goal in GR. It is defined as the multilinear function which gives the magnitude of a vector. More exactly $g(u,v) = g_{uv}dx^u(u) dx^v(v) \equiv u \cdot v$ for vectors $v,u$. If $ v = u$, $g(v,v) = g_{uv}dx^u(v) dx^v(v) \equiv |v|^2$. Therefore, the metric is a tensor of rank $(0,2)$ formed by the tensor product of two arbitrary dual vectors. This defintion is very natural since, for example, in Euclidean space, the magnitude of a vector is defined as $|v|^2 = \sum_\alpha(v^\alpha)^2 = v^\alpha v_\alpha$ The implict metric that is used in Cartesian coordinates is $g_{uv} = \delta_{uv}$. This means $v^\alpha v_\alpha = g_{\alpha \beta}v^\alpha v^\beta$. Note this defines a natural 1-1 correspondance between vectors and dual vectors $g(\, ,v) = g_{uv}dx^udx^v(v) = g_{uv}dx^udx^v(v^\alpha \partial_\alpha) = g_{uv}v^\alpha dx^u dx^v(\partial_\alpha) =  g_{uv}v^\alpha dx^u \delta^v_\alpha = g_{uv}v^vdx^u \equiv v_u dx^u = v^*$</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=3ea31f5c-824f-4f87-a2fd-ae4ea20819b1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
</div>
</div>
</div>
</div>
</main>
</body>
</html>
