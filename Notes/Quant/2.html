<!DOCTYPE html>



<html lang="en">

<head><meta charset="utf-8"/>

    <meta content="width=device-width, initial-scale=1.0" name="viewport"/>

    <title>Post 2</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>



    <link rel="stylesheet" href="../../style.css">

<script src="../../marked.min.js"></script>

<script src ='../../script.js'></script>

<!-- Load mathjax -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>

<script type="text/x-mathjax-config">

    init_mathjax = function() {

        if (window.MathJax) {

        // MathJax loaded

            MathJax.Hub.Config({

                TeX: {

                    equationNumbers: {

                    autoNumber: "AMS",

                    useLabelIds: true

                    }

                },

                tex2jax: {

                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],

                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],

                    processEscapes: true,

                    processEnvironments: true

                },

                displayAlign: 'center',

                messageStyle: 'none',

                CommonHTML: {

                    linebreaks: {

                    automatic: true

                    }

                }

            });



            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);

        }

    }

    init_mathjax();

</script>

<!-- End of mathjax configuration -->

<body>

<h1 id="Chapter-2" style="align-self: baseline">Post 2</h1>
<div class="display math">(Note: This is based on Chapter 2 of Statistical Inference, Casella and Burger)

If X is a random variable with CDF $F_X(x)$, then any function of X, $g(X)$ is also a random variable. The probabilistic behavior of $Y = g(X)$ can be described in terms of $X$ meaning $P(Y \in A) = P(g(X) \in A)$ for any set $A$. In more detail if $y = g(x)$, $\chi$ is the sample space of $X$ and $\mathcal Y$ is the sample space of $Y$, $g(x): \chi \rightarrow \mathcal Y$, with an inverse mapping $g^{-1}(A = \{ x \in X: g(x) \in A\} \,$.

If $Y = g(X)$, $P(Y\in A) = P(X \in g^{-1}(A))$. This is the probability distribution of Y which can be shown to satisfy the Kolmogorov Axioms.

If X is a discrete random variable, $\chi$ is also countable. The sample space $Y = g(X)$ is also a countable set, meaning $Y$ is a discrete random variable as well. $f_Y(y) = P(Y = y) = \sum_{x \in g^{-1}(y)} P(X = x) = \sum{_x \in g^{-1}(y)}f_X(x)$ for $y \in \mathcal Y$ and 0 otherwise, for $y \notin \mathcal Y$. Then, finding the PMF of Y means to simply find $g^{-1}(y)$

ex) A discrete random variable X has a binomial distribution if its PMF is $f_X(x) = P(X = x) = \binom{n}{x}p^x(1-p)^{n-x}$ for $x \in 0,1,...,n$ where $n$ is a positive integer and p ranges from 0 to unity Values such as n and p can be set to different values and produce different probability distributions are called parameters.

If $g(x) = n -x, Y = g \Rightarrow Y = n - X$. $\mathcal Y = \{ y: y = g(x), x \in \chi \} = {0,1,...,n}$. Then, $g^{-1}(y) = n - y$, which means $f_Y(y) = \sum_{x \in g^{-1}(y)} f_X(x) = f_{X}(n-y) = \binom{n}{n-y}p^{n-y}(1-p)^{n-(n-y)} = \binom{n}{y}(1-p)^yp^{n-y}$. $Y$ is also binomial distribution but with parameters n, which is the same, and $1-p$ which is different

For a continuous distribution, $F_Y(y) = P(Y \leq y) = P(g(X) \leq y) = P(\{x \in \chi: g(x) \leq y\}) = \int_{\{x \in \chi: g(x) \leq y\}}f_X(x)dx$. However, the region of integration is not always easy to identify.

ex) Uniform transformation
$\chi = \{x: f_X(x) > 0\},\mathcal Y = \{Y: y = g(x) \text{ for some } x \in \chi\}$. The pdf of the random variable X is positive only on the set $\chi,$ but 0 elsewhere, called the support set of a distribution.

The best functions $g(x)$ are those that are monotone, meaning they are either increasing or decreasing, $u > v \Rightarrow g(u) > g(v)$, or $u < v \Rightarrow  g(u) < g(v)$. If this is true, then $g: x \rightarrow y = g(x)$ forms a bijection from $\chi \rightarrow \mathcal Y$.

Theorem: Let X have CDF $F_X(x)$, $Y=g(X)$, and $\chi,\mathcal Y$ are the corresponding domain and range of $g$.
    a) If g is an increasing function on $\chi$, $F_Y(y) = F_X(g^{-1}(y)), \text{ for } y \in \mathcal Y$
    b) If g is a decreasing function on $\chi$ and X is a continuous random variable $F_Y(y) = 1 - F_X(g^{-1}(y))$ for $y \in \mathcal Y$

ex) If for X, $f_X(x) = 1, 0 < x < 1 \Rightarrow F_X(x) = x, 0 < x < 1$. Then, define the transformation $Y = g(X) = -\ln x$. Since $\frac{d}{dx}g(x) = \frac{-1}{x} < 0, 0 < x < 1$, g(x) is a decreasing function. $X \in (0,1), Y \in (0,\infty)$. Also, $g^{-1}(y) = e^{-y} \Rightarrow y>0, F_Y(y) = 1 - F_X(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y} {}$

Theorem: Let X have pdf $f_X(x), Y = g(X)$, g is a monotone function. Let $\chi,\mathcal Y$ be the domain and range of g. $f_X(x)$ is continuous on $\chi$ and $g^{-1}(y)$ has a continuous derivative on $\mathcal Y$ means $f_Y(y) = \begin{cases}f_X(g^{-1}(y)|\frac{d}{dy})g^{-1}(y)| & y \in \mathcal Y \\ 0 & \text {else} \end{cases}$

ex) Inverted gamma pdf $f_X(x) = \frac{1}{(n-1)!\beta^n}x^{n-1}e^{-x/\beta}, 0 < x < \infty$, where $\beta > 0, n $ is a positive integer. If $g(X) = 1/X$, with the support sets being the same for $\chi= \mathcal Y = (0,\infty), y = g(x) \Rightarrow g^{-1}(y) = 1/y \Rightarrow \frac{d}{dy}g^{-1}(y) = -1/(y^2)$ means that $f_Y(y) = f_X(g^{-1}(y))|\frac{d}{dy}g^{-1}(y)| = \frac{1}{(n-1)!\beta^n}(1/y)^{n-1}e^{-1/y\beta}(1/y^2) = \frac{1}{(n-1)!\beta^n}(1/y)^{n+1}e^{-1/y\beta}$

A lot of the time g is neither increasing nor decreasing $x \in \chi$, but over certain intervals or a finite sum of such intervals.

ex) Square transformation where for a continuous random variable $X$, $Y = X^2$ has the CDF $F_Y(y) = P(Y \leq y) = P(X^2 \leq y) = P(-y^{1/2} \leq X \leq y^{1/2}) = P(X \leq y^{1/2}) - P(X \leq -y^{1/2}) = F_X(y^{1/2}) - F_X(-y^{1/2})$ where x having continuity implies the equality on the left endpoint could be removed $f_Y(y) = \frac{d}{dy}F_Y(y) = \frac{1}{2y^{1/2}}(f_X(y^{1/2}) + f_X(-y^{1/2}))\,$. This is the sum of the two pieces of the interval where $g(x)$ is monotone.

ex) Normal-chi squared relationship. Let X have the standard normal distribution $f_X(x) = (1/(2\pi)^{1/2})e^{-x^2/2},-\infty < x < \infty,$. If $Y = X^2$, $\chi$ can be split into two monotone intervals $(-\infty,0),(0,\infty)$ with $\mathcal Y = (0,\infty),$then $f_Y(y) =f_X(g_1^{-1}(y))|\frac{d}{dy}g^{-1}(y)| + f_X(g_2^{-1}(y))|\frac{d}{dy}g^{-1}(y)|$ with $g_1^{-1}(y) = -y^{1/2}, g_2^{-1}(y) = y^{1/2}$ meaning $\,f_Y(y) = (1/(2\pi)^{1/2})(e^-{(-(y^{1/2})^2)/2}|-1/(2y^{1/2})| + (1/(2\pi)^{1/2})(e^-{((y^{1/2})^2)/2}|1/(2y^{1/2})| = (1/(2y\pi)^{1/2})e^{-y/2}$.
The pdf of Y is one of a chi squared random variable with 1 degree of freedom

Theorem: Probability integral transformation
Let X have continuous CDF $F_X(x), Y = F_X(X)$. Then, $Y$ is uniformly distributed on $y,(0,1), P(Y \leq y) = y$

Used to generate random samples from a particular distribution.

The expected value or mean of a random variable $g(X)$, $Eg(X) = \begin{cases} ,\int_{-\infty}^\infty g(x) f_X(x)dx & \text{cont}\\ \sum_{x \in \chi}g(x)f_X(x) & \text{discrete} \end{cases}$. If $E|g(X)| \rightarrow \infty$, say that $Eg(X)$, DNE.

ex) Exponential distribution, pdf $f_X(x) = (1/\lambda)e^{-x/\lambda}, 0 \leq x \leq \infty, \lambda > 0$. Then $EX = \int_0^\infty (1/\lambda)xe^{-x/\lambda}dx = -xe^{-x/\lambda}|_0^\infty - \int_0^\infty -e^{-x/\lambda}dx = \lambda$

ex) Binomial distribution, pdf $f_X(x) = P(X = x) = \binom{n}{x}p^x(1-p)^{n-x}, x = 0,1,..,n, 0 \leq p \leq 1$. Then, $EX = \sum_{x=0}^nx \binom{n}{x}p^x(1-p)^{n-x} = \sum_{x=1}^nx \binom{n}{x}p^x(1-p)^{n-x}$. Note the identity $x\binom{n}{x} = x \frac{n!}{x!(n-x)!} = n \frac{(n-1)!}{(x-1)!((n-1)-(x-1)!)} = n \binom{n-1}{x-1}$ which makes $EX = \sum_{x=1}^nn \binom{n-1}{x-1}p^x(1-p)^{n-x}$. With $y = x - 1 \Rightarrow \sum_{y=0}^{n-1}n \binom{n-1}{y}p^{y+1}(1-p)^{n-(y+1)} = np\sum_{y=0}^{n-1} \binom{n-1}{y}p^y(1-p)^{(n-1) -y}$. The sum is just that of the pdf of a binomial distribution with parameters $n-1,p$, meaning it equals - Therefore, $EX = np$

ex) A Cauchy random variable has a mean which does not exist. It has a pdf $f_X(x) = (1/\pi)\frac{1}{1+x^2},-\infty < x < \infty$. Since $\int_0^\infty\frac{1}{1+x^2}dx,x = \tan \theta, dx = \sec^2\theta d\theta \Rightarrow \int_0^{\pi/2}\frac{1}{1+\tan^2\theta}\sec^2\theta d\theta = \int_0^{\pi/2}d \theta = \pi/2$, this means $\int_{-\infty}^{\infty}f_X(x)dx =(1/\pi)(\pi) =  1$ but $E|X| = \int_{-\infty}^{\infty}(1/\pi)\frac{|x|}{1+x^2}dx = (2/\pi)\int_0^{\infty}\frac{x}{1+x^2}dx = (1/\pi)\ln(1+x^2)|_0^\infty \rightarrow \infty$

The operation of taking expectations is linear, meaning for constants a and b $E(aX +b) = aEX + b$. An example is if the pdf of X is a binomial distribution with parameters $n,p$ $E(X - np) = EX - np = 0$.

Theorem: Let X be a random variable and $a,b,c$ be constants. Then for any functions $g_1(x),g_2(x),$ with existing expectation values
a) $E(ag_1(X) + bg_2(X) + c) = aEg_1(X) + bEg_x(X) + c$
b) If $g_1(x) \geq 0$ for all x, then $Eg_1(X) \geq 0$
c) If $g_1(x) \geq g_2(x)$ for all x, then $Eg_1(X) \geq Eg_2(X)$
d) If $a \leq g_1(x) \leq b$ for all x, then $\, a\leq Eg_1(x) \leq b$.

ex) Minimizing the distance of a random variable from a given constant b is also an important property of the mean. Say the goal is to find b such that $E((X - b)^2)$ is minimized, meaning that b would be a good predictor of X. $E((X - b)^2) = E(X^2 -2Xb + b^2) = E(X^2) -2bE(X) + b^2$.
Then, $\frac{d}{db}E((X - b)^2) = 2b + \frac{d}{db}E(X^2) - 2E(X) - 2b\frac{d}{db}E(X) = 0 \Rightarrow b + (1/2)\frac{d}{db}\int_{-\infty}^{\infty}x^2f_X(x)dx - \int_{-\infty}^{\infty}xf_X(x)dx - b\frac{d}{db}\int_{-\infty}^{\infty}xf_X(x)dx =b + \int_{-\infty}^{\infty} dx((1/2)\frac{d}{db}(x^2f(x)) -xf(x) -xb\frac{d}{db}f)$

An easier way to do this is to note that $E((X -b)^2) = E((X - EX + EX - b)^2) = E((X - EX)^2) + (EX - b)^2 + 2E((X-EX)(EX-b))$ and since $E((X-EX)(EX-b) = E(X(EX) -bX -(EX)^2+bEX) = 0$, then $E((X-b)^2) =E((X - EX)^2) + (EX - b)^2\,$. If $b = EX$ this quantity is minimized.

When finding the expectation value of a function of a random variable, there are two ways to proceed.
$Eg(x) = \int_{-\infty}^\infty g(x) f_X(x)dx$
$Eg(x) = EY = \int_{-\infty}^\infty y f_Y(y)dy$

ex) X has an uniform distribution with pdf $f_X(x) = \begin{cases} 1 & 0 \leq x \leq 1 \\ 0 & \text{else} \end{cases} \,$ and if $g(X) = - \ln x \Rightarrow Eg(X) = \int_0^1(1)(-\ln x) dx = -x \ln x |_0^1+ \int_0^1 x(1/x)dx = 1$. Another way is $Y = \ln X, f_Y(y) = e^{-y}$ from before, meaning $\int_0^\infty y e^{-y}dy = 1$

The nth moment of X for every integer n $u'_n = EX^n$.

The nth central moment of X $u_n = E(X - u)^n, u = u'_1 = EX$

The variance of a random variable X is its second central moment $Var X = E(X - EX)^2$. The positive square root of Var X is the standard deviation of X.

ex) For the exponential distribution, $EX = \lambda, Var X = E(X - \lambda)^2 = \int_0^\infty (x - \lambda)^2 (1/\lambda)e^{-x/\lambda}dx = \int_0^\infty dx(x^2 + \lambda^2 - 2 x \lambda)e^{-x/\lambda} = \lambda^2 + A -2\lambda B$. Since $A = \int_0^\infty dx(x^2e^{-x/\lambda}), u = x^2, v = -\lambda e^{-x/\lambda} \Rightarrow A = x^2(-\lambda e^{-x/\lambda})|_0^\infty +\lambda 2\int_0^\infty x e^{-x/\lambda} dx = 0 + 2 \lambda B\,$, then $Var X = \lambda^2 + 2\lambda B - 2 \lambda B = \lambda^2$

If X is a random variable with finite variance for any constants a,b, $Var(aX + b) = a^2 Var X$. This is because $Var(aX + b) = E(aX+b - E(aX+b))^2 = E(aX - a EX)^2 = E(a(X - EX))^2 = E(a^2(X - EX)^2)= a^2 Var X$

Another formula for the variance $Var X = E(X - EX)^2 = E(X^2 - 2(EX) X + (EX)^2) = E(X^2) -2(EX)^2 + (EX)^2 = E(X^2) - (EX)^2$

ex) For a binomial distribution with pdf $P(X = x) = \binom{n}{x}p^x(1-p)^{n-x}, x = 0,1,...,n, EX = np$. Then, $EX^2 = \sum_{x=0}^nx^2\binom{n}{x}p^x(1-p)^{n-x}$. With $x\binom{n}{x} = n\binom{n-1}{x-1}$, $EX^2 = n\sum_{x=1}^nx\binom{n-1}{x-1}p^x(1-p)^{n-x}, y = x - 1 \Rightarrow EX^2 = n\sum_{x=1}^{n-1}(y+1)\binom{n-1}{y}p^{y+1}(1-p)^{n-y-1}= (np)(\sum_{x=1}^{n-1}(y)\binom{n-1}{y}p^y(1-p)^{n-y-1}+ \sum_{x=1}^{n-1}\binom{n-1}{y}p^y(1-p)^{n-y-1}) = np((n-1)(p) + 1) = n(n-1)p^2 + np$. Then, $Var X = EX^2 - (EX)^2 = n(n-1)p^2 + np - (np)^2 = np -np^2 = np(1-p)$

Usually only up to this is needed to be calculated, maybe a 3rd or 4th moment.

Another function associated with a probability distribution or random variable X is the moment generating function MGF
It's defined as $M_X(t) = Ee^{tX}$ as long as this quantity, meaning the expectation, exists for t in some neighborhood of 0.

For a continuous variable, $M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x) dx$

For a discrete variable, $M_X(t) = \sum_x e^{tx} P(X=x)$.

Theorem: If X has MGF $M_X(t)$,$EX^n = M_X^{(n)}(0)$ where $M_X^{(n)}(0) = \frac{d^n}{dt^n}M_X(t)|_{t=0}$. The nth moment is equal to the nth derivative of $M_X(t)$ evaluated at t = 0.
Note that using the operator expansion of the exponential $M_X(t) = E(1 + tX + (1/2!)(tX)^2+...) = u'_0 + tu'_1 + (t^2/2!)u'_2+...=\sum_{i=0}^\infty\frac{t^i u'_i}{i!}$. The theorem is easily seen from this.

ex) The gamma pdf is defined as $f_X(x) =\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha - 1}e^{-x/\beta}, 0 < x < \infty, \alpha > 0, \beta > 0 {}$. This means $M_X(t) = \int_0^\infty dx e^{tx}\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha - 1}e^{-x/\beta} = \frac{1}{\Gamma(\alpha)\beta^\alpha}\int_0^\infty dx e^{-((1/\beta) -t)}x^{\alpha - 1}  \,$. Note that this is just another gamma pdf with a different $\beta \rightarrow \frac{\beta}{1-\beta t}$, meaning $1 = \int_0^\infty dx\frac{1}{\Gamma(\alpha)(\frac{\beta}{1-\beta t})^\alpha}x^{\alpha - 1}e^{-x/\frac{\beta}{1-\beta t}} \Rightarrow \Gamma(\alpha)(\frac{\beta}{1-\beta t})^\alpha =  \int_0^\infty dx x^{\alpha - 1}e^{-x/\frac{\beta}{1-\beta t}}$. This means $M_X(t) = \frac{1}{\Gamma(\alpha)\beta^\alpha}\Gamma(\alpha)(\frac{\beta}{1-\beta t})^\alpha = (\frac{1}{1-\beta t})^\alpha$ which is true note $\frac{\beta}{1-\beta t} > 0 \Rightarrow \frac{1}{1/\beta -t} > 0 \Rightarrow 1/\beta -t > 0\Rightarrow 1/\beta > t$ . The integrand diverges otherwise, meaning the MGF cannot be defined in those situations.h

$EX = \frac{d}{dt}M_X(t)|_{t=0} = (-\alpha)(-\beta)1/(1-\beta t)^{\alpha + 1}|_{t=0} = \alpha \beta,$ then with all other moments being calculated in a similar way

ex) binomial distribution has MGF $M_X(t) = \sum_{x=0}^ne^{tx} \binom{n}{x}p^x(1-p)^{n-x} = \sum_{x=0}^n \binom{n}{x}(pe^t)^x(1-p)^{n-x}$. The binomial formula $\, \sum_{x=0}^n\binom{n}{x}u^xv^{n-x} = (u+v)^n$ implies $M_X(t) = (pe^t +1-p)^n$

The specification of all the moments of a pdf does not uniquely define the pdf since it is possible for two random variables to have the same moments.

Theorem: Let $F_X(x),F_Y(y)$ be two CDFs all of whose moments exist.
a) If X and Y have bounded support, then $F_X(u) = F_Y(u)$ for all u if and only if $EX^r = EY^r$ for all natural numbers r.
b) If the moment generating functions exist and $M_X(t) =M_Y(t)$ for all t in some neighborhood of 0, then $F_X(u) = F_Y(u)$ for all u

Theorem: Concerning the convergence of MGFs
Suppose $\{X_i, i= 1,2,...\}\,\,$ is a sequence of random variables, each with MGF $M_{X_i}(t)$. Also if $\lim_{i \rightarrow \infty} M_{X_i}(t) = M_X(t)$ for all $t$ in a neighborhood of 0 and $M_X(t)$ is an MGF of some random variable $X$. Then there is an unique CDF $F_X$ whose moments are determined by $M_X(t)$ and for all x where $F_X(x)$ is continuous, $\lim_{i \rightarrow \infty}F_{X_i}(x) = F_X(x)$ meaning convergence of MGFs to an MGF implies convergence of CDFs

The proof for the last two theorems is technical, so it is considered definitely reasonable. The Laplace transform of $M_X(t)$ is $f_X(x)$ actually since $M_X(t) = \int_{-\infty}^\infty e^{tx}f_X(x)dx$ and an important fact about Laplace transforms is their uniqueness in a neighborhood of t where the Laplace transform is valid. There is a 1-1 correspondence then.

ex) The Poisson approximation is used for binomial probabilities where the Poisson PMF is $P(X = x) = \frac{e^{-\lambda}\lambda^x}{x!}, x = 0,1,2..., \lambda > 0$ a positive constant. If X is a binomial PMF and Y is a Poisson PMF, with n being large and defining $\lambda = np$ being small, then $P(X = x) \approx P(Y =x)$
This is fine by showing their MGF convergence with $M_X(t) = (pe^t +1-p)^n, M_Y(t) = e^{\lambda(e^t -1)}$
$M_Y(t) = \sum_{y = 0}^\infty e^{ty}\frac{e^{-\lambda}\lambda^y}{y!} = e^{-\lambda} \sum_{y = 0}^\infty \frac{(e^t \lambda)^y}{y!} = e^{-\lambda}e^{e^t \lambda} = e^{\lambda(e^t - 1)}$.

Also, this result of the exponential is needed. Let $\{a_i\},i = 1,2,...$ be a sequence of numbers converging to a, $\lim_{n \rightarrow \infty}a_n = a$. Then, $\lim_{n \rightarrow \infty}(1 + \frac{a_n}{n})^n = e^a$.
It can be shown as follows. Defining $\, L = \lim_{n \rightarrow \infty}(1 + \frac{a_n}{n})^n \Rightarrow \ln L = \ln(\lim_{n \rightarrow \infty}(1 + \frac{a_n}{n})^n) = \lim_{n \rightarrow \infty}(\ln(1 + \frac{a_n}{n})^n)$$\,=\lim_{n \rightarrow \infty}n \ln(1 + \frac{a_n}{n})  =\lim_{n \rightarrow \infty}\frac{\ln(1 + \frac{a_n}{n})}{1/n}=\lim_{n \rightarrow \infty}\frac{n/(a_n + n)(-1/n^2)(a'_n)}{-1/n^2}= \lim_{n \rightarrow \infty} \frac{na'_n}{a_n + n}$$=\lim_{n \rightarrow \infty} \frac{a'_n}{1} = a'_n = a' \Rightarrow L = e^{a'}$. Note that the slightly tricky part is taking the derivative of the sequence with respect to n if it is considered as a continuous variable. Since it is not really defined, it is fine to merely treat it as a constant in the sense that one can imagine writing the whole sequence out $\lim_{n \rightarrow \infty} \frac{nan}{a_n + n}$

Returning to the task $M_X(t) = (1 + p(e^t - 1))^n  = (1 + \frac{(np)(e^t - 1)}{n})^n =(1 + \frac{(\lambda( e^t - 1)}{n})^n$ since $\lambda = np$ is held fixed for a given Poisson dist. Taking the limit such that $\lim_{n \rightarrow \infty} M_{X_n}(t)$ with taking n as discrete variable to define the sequence $M_{X_n}(t)$, then $\lim_{n \rightarrow \infty} M_{X_n}(t) = \lim_{n \rightarrow \infty}(1 + \frac{\lambda (e^t - 1)}{n})^n = e^{\lambda (e^t - 1)} = M_Y(t)$. Since the limit was only valid in the limit of the numerator being small within the expression for the power and $n$ being large, this approximation does make sense in these situations.

Theorem: For any constants a,b with a random variable $X, M_{aX +b}(t) = e^{bt}M_X (at)$
Proof:$M_{aX +b}(t) = Ee^{(aX +b)t} = e^{bt}Ee^{at X} = e^{bt} M_X (at)$

An important process is to differentiate under the integral sign meaning when does the process of integration commute with differentiation.

Note there will not be any detailed proofs since that is better suited for a real analysis study but the important results will be examined.

$\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)} f(x,\theta) dx \,$ where $-\infty< a,b < \infty$
Theorem: Leibnitz's Rule
If $f(x,\theta),a(\theta),b(\theta)$ are differentiable with respect to $\theta$

This question is merely if the process of taking a limit and integration can be interchanged, $\frac{\partial}{\partial\theta}f(x,\theta) = \lim_{\delta \rightarrow 0}\frac{f(x,\theta + \delta) -f(x,\theta)}{\delta} \Rightarrow \int_{-\infty}^\infty\frac{\partial}{\partial\theta}f(x,\theta) dx = \int_{-\infty}^\infty \lim_{\delta \rightarrow 0}\frac{f(x,\theta + \delta) -f(x,\theta)}{\delta} dx ,$

Theorem: Suppose the function $h(x,y),x,y$ is continuous at $y_0$ for each $x$ and there exists a function $g(x)$ satisfying
    i. $|h(x,y)| \leq g(x)$ for all $x,y$
    ii. $\int_{-\infty}^\infty g(x) dx < \infty$

then, $\lim_{y \rightarrow y_0}\int_{-\infty}^\infty h(x,y) dx =\int_{-\infty}^\infty \lim_{y \rightarrow y_0}h(x,y) dx \,$.
This ensures that essentially the integral does not diverge to infinity which allows for the interchange since the integrating variable and the variable the limit is being taken on are independent

Applying this theorem to the derivative definition means to assume such a function $g(x,\theta_0)$ exists for every x such that $|\frac{f(x,\theta + \delta) -f(x,\theta)}{\delta}| \leq g(x,\theta_0)$ for all $x, |\delta| \leq \delta_0$ where $\delta_0 >0,$ meaning the limiting value has to exist in this neighborhood essentially around the point $\theta_0$. Then with the additional condition $\int_{-\infty}^\infty g(x,\theta_0) dx < \infty$, then $\,\lim_{\delta \rightarrow 0}\int_{-\infty}^\infty \frac{f(x,\theta + \delta) -f(x,\theta)}{\delta} dx = \int_{-\infty}^\infty \lim_{\delta \rightarrow 0}\frac{f(x,\theta + \delta) -f(x,\theta)}{\delta} dx$$\Rightarrow \frac{d}{d \theta}(\int_{-\infty}^\infty f(x,\theta)dx)|_{\theta =\theta_0} = \int_{-\infty}^\infty (\frac{\partial}{\partial \theta}f(x,\theta)dx)|_{\theta =\theta_0} dx \,$

This is called a Lipschitz condition, which imposes smoothness on a function

As long as this applies in a range of $\theta$, the distinction between $\theta, \theta_0$ is not needed to be looked at.

If differentiability is true for all $\theta$, the condition $|\frac{f(x,\theta + \delta) -f(x,\theta)}{\delta}| \leq g(x,\theta_0)$ can just be taken to be $\frac{\partial}{\partial \theta}f(x,\theta)|_{\theta = \theta'} \leq g(x,\theta)$, for all $\theta', |\theta' - \theta| \leq \delta_0$

ex) The exponential pdf $f(x) = (1/\lambda)e^{-x/\lambda}, 0 < x < \infty$. IF wish to find $\frac{d}{d\lambda}EX^n = \frac{d}{d \lambda}\int_0^\infty x^n(1/\lambda)e^{-x/\lambda}dx = \int_0^\infty \frac{d}{d \lambda}(x^n(1/\lambda)e^{-x/\lambda})dx = \int_0^\infty x^n ((-1/\lambda^2)e^{-x/\lambda} + 1/\lambda(x/\lambda^2)e^{-x/\lambda}) dx \,$$\, = \int_0^\infty x^n(1/\lambda^2)e^{-x/\lambda} (x/\lambda - 1) dx = (1/\lambda^2) E X^{n +1} - ( 1/\lambda)EX^n$

Proof of exchange being allowed
The recursion relation $\frac{d}{d\lambda}EX^n = (1/\lambda^2) E X^{n +1} - ( 1/\lambda)EX^n \Rightarrow \lambda^2\frac{d}{d\lambda}EX^n =  E X^{n +1} - \lambda EX^n$

ex) The normal distribution with mean u, variance 1, pdf $f(x) = \frac{1}{(2\pi)^{1/2}}e^{-(x-u)^2/2}$
$EX = \int_{-\infty}^\infty dx x f(x) =  \int_{-\infty}^\infty dx x\frac{1}{(2\pi)^{1/2}}e^{-(x-u)^2/2}, y = x - u, dy = dx \Rightarrow \int_{-\infty}^\infty dy (y+u)\frac{1}{(2\pi)^{1/2}}e^{-y^2/2} = u\frac{1}{(2\pi)^{1/2}}(2\pi)^{1/2} = u$.
$E(X - EX)^2 = E(X^2) - (EX)^2$
$\frac{d}{d u}EX^n = \frac{d}{du} \int_{-\infty}^\infty dx x^n\frac{1}{(2\pi)^{1/2}}e^{-(x-u)^2/2}$

The interchange of summation and differentiation has certain conditions for com as well.

ex) The geometric distribution $P(X=x) = \theta(1-\theta)^x, x = 0,1,..., 0 < x <1 ,\frac{d}{d\theta}\sum_{x=0}^\infty  \theta(1-\theta)^x = 0$ since the sum is just - Also, $\frac{d}{d\theta}\sum_{x=0}^\infty  \theta(1-\theta)^x =\sum_{x=0}^\infty  \frac{d}{d\theta}\theta(1-\theta)^x = \sum_{x=0}^\infty ((1-\theta)^x - \theta x (1-\theta)^{x-1}) \,$$= (1/\theta)\sum_{x=0}^\infty \theta (1-\theta)^x - (1/(1-\theta))\sum_{x=0}^\infty\theta x (1-\theta)^x = 0$ $\Rightarrow(1/\theta)\sum_{x=0}^\infty \theta (1-\theta)^x = (1/(1-\theta))\sum_{x=0}^\infty\theta x (1-\theta)^x \Rightarrow (1/\theta) - (1/(1-\theta)) EX = 0 \Rightarrow EX = \frac{1-\theta}{\theta}$

Theorem: Summation and Differentiation commutation
Supposed the series $\sum_{x=0}^\infty h(\theta,x)$ converges for all $\theta$ in an interval $(a,b)$ of real numbers and
$\frac{\partial}{\partial\theta}h(\theta,x)$ is continuous in $\theta$ for each x
and $\sum_{x=0}^\infty \frac{\partial}{\partial \theta} h(\theta,x)$ converges uniformly on every closed bounded subinterval of $(a,b)$.
Then, $\frac{d}{d\theta} \sum_{x=0}^\infty = \sum_{x=0}^\infty \frac{\partial}{\partial\theta} h(\theta,x)$

Uniform convergence is the most important property.

ex) Continued to prove for the geometric distribution
$h(\theta,x) = \theta(1-\theta)^x\Rightarrow \frac{\partial}{\partial\theta}h(\theta,x) = (1-\theta)^x - \theta x (1-\theta)^{x-1}$. Defining $S_n(\theta) =\sum_{x=0}^n (1-\theta)^x - \theta x (1-\theta)^{x-1}, [c,b] \subset (0,1)$ will have uniform con if for all $\epsilon > 0,$ there is$n > N \Rightarrow |S_n(\theta) - S_\infty(\theta) < \epsilon$ for the given range of $\theta$ def $\theta \in [c,d]$, meaning $\sum_{x=0}^n (1-\theta)^x = \frac{1-(1-\theta)^{n+1}}{\theta}, \sum_{x=0}^n \theta x (1-\theta)^{x-1}  = \theta \sum_{x=0}^n  -\frac{\partial}{\partial\theta}(1-\theta)^{x} = -\theta \frac{d}{d\theta} \sum_{x=0}^n(1-\theta)^{x}$$\, =\theta(1/\theta^2)(-(-(n+1)\theta(1-\theta)^n + (1-(1-\theta)^{n+1})) \Rightarrow S_n(\theta)  =(n+1)(1-\theta)^n$  which is true for all positive integers $n$ given that $\theta$ is a positive number less than 1

Theorem: Summation and Integration commutation: Same .

</div>

</body></html>